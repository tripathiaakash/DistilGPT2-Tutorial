{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Fine-Tune DistilGPT2 and Generate Text",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C9zObeOoEoN"
      },
      "source": [
        "\n",
        "Here is a tutorial about generating text using a SOTA inspired language generation model, distilgpt2. This model lighter in weight and faster in language generation than the original OpenAI GPT2. Using this tutorial, you can train a language generation model which can generate text for any subject in English. Here, we will generate movie reviews by fine-tuning distilgpt2 on a sample of IMDB movie reviews.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW8OfkKEhPPu"
      },
      "source": [
        "Click on the link below and a file will be downloaded containing IMDB sample dataset of 1000 samples\n",
        "\n",
        "http://files.fast.ai/data/examples/imdb_sample.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1bteQwqhsUf"
      },
      "source": [
        "Upload this file in this colab notebook using the upload button on the top left "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCqeIu66WrOJ"
      },
      "source": [
        "### Extract the csv file from the uploaded tgz file\n",
        "\n",
        "import tarfile\n",
        "with tarfile.open('imdb_sample.tgz', 'r:gz') as tar:\n",
        "    tar.extractall()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziTUQXO7Yxve"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye9GZEhpZRtz"
      },
      "source": [
        "data = pd.read_csv('imdb_sample/texts.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSU8OLAhZada",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "a6a0ade1-3786-4d3f-c71b-e90318d2566c"
      },
      "source": [
        "### This is how the CSV look like\n",
        "data"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>is_valid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>negative</td>\n",
              "      <td>Un-bleeping-believable! Meg Ryan doesn't even ...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>This is a extremely well-made film. The acting...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>negative</td>\n",
              "      <td>Every once in a long while a movie will come a...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>positive</td>\n",
              "      <td>Name just says it all. I watched this movie wi...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negative</td>\n",
              "      <td>This movie succeeds at being one of the most u...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>negative</td>\n",
              "      <td>There are many different versions of this one ...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>positive</td>\n",
              "      <td>Once upon a time Hollywood produced live-actio...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>negative</td>\n",
              "      <td>Wenders was great with Million $ Hotel.I don't...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>negative</td>\n",
              "      <td>Although a film with Bruce Willis is always wo...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>positive</td>\n",
              "      <td>A compelling, honest, daring, and unforgettabl...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        label                                               text  is_valid\n",
              "0    negative  Un-bleeping-believable! Meg Ryan doesn't even ...     False\n",
              "1    positive  This is a extremely well-made film. The acting...     False\n",
              "2    negative  Every once in a long while a movie will come a...     False\n",
              "3    positive  Name just says it all. I watched this movie wi...     False\n",
              "4    negative  This movie succeeds at being one of the most u...     False\n",
              "..        ...                                                ...       ...\n",
              "995  negative  There are many different versions of this one ...      True\n",
              "996  positive  Once upon a time Hollywood produced live-actio...      True\n",
              "997  negative  Wenders was great with Million $ Hotel.I don't...      True\n",
              "998  negative  Although a film with Bruce Willis is always wo...      True\n",
              "999  positive  A compelling, honest, daring, and unforgettabl...      True\n",
              "\n",
              "[1000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y55Nj02it0c"
      },
      "source": [
        "Let's get the number of samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-85131-nifk5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "243f1926-13fd-4148-cdd4-d7d038dd8fef"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRvpRcHai0qa"
      },
      "source": [
        "For Finetuning distilgpt2, we just need the text field"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAbMLfxrZbKG"
      },
      "source": [
        "texts = list(set(data['text']))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buC-SCx-Zkty",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54d532ed-f658-45f4-9a03-32d3e95e71c3"
      },
      "source": [
        "len(texts)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUXGPS5Li-E4"
      },
      "source": [
        "Store the reviews in a txt file where each line of txt file is a single review "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LotaG9qgZmHy"
      },
      "source": [
        "file_name = 'testing.txt'\n",
        "with open(file_name, 'w') as f:\n",
        "    f.write(\" |EndOfText|\\n\".join(texts))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KwFvrFRjOwo"
      },
      "source": [
        "Now, let's come to Transformers by Huggingface, and unleash the Transformers (Autobots... just kidding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkocIBHfaZul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2074166-fdf9-4e73-fe97-2e2fe47cdbe7"
      },
      "source": [
        "!pip install transformers\n",
        "!git clone https://github.com/huggingface/transformers.git"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.5MB 22.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 50.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.9MB 50.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=a456f8cc9444894e38f1f465a93a5ca651eb94dffff8af42e5331d78f2d46296\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.1.1\n",
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 58615, done.\u001b[K\n",
            "remote: Total 58615 (delta 0), reused 0 (delta 0), pack-reused 58615\u001b[K\n",
            "Receiving objects: 100% (58615/58615), 43.78 MiB | 28.54 MiB/s, done.\n",
            "Resolving deltas: 100% (41211/41211), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbIJfTnDjmG6"
      },
      "source": [
        "Make 2 directories. \n",
        "\n",
        "1) weights - for storing the weights of distilgpt2\n",
        "\n",
        "2) tokenizer - for storing the tokenizer of distilgpt2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhH0xD1-qnh-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d899b417-d2fa-4ae4-ffb5-81e09baff462"
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/78/5873ac1e27bf25a2cbf3447d6704edd3136b1b3ff0eb3bfab38a45d2a1ff/datasets-1.2.0-py3-none-any.whl (159kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 163kB 11.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.19.4)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n",
            "Collecting pyarrow>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e1/27958a70848f8f7089bff8d6ebe42519daf01f976d28b481e1bfd52c8097/pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.7MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.7MB 215kB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 245kB 57.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.8)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: pyarrow, xxhash, datasets\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "Successfully installed datasets-1.2.0 pyarrow-2.0.0 xxhash-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1HiZgs-kFLg"
      },
      "source": [
        "Now, its time for Training (or fine tuning) distilgpt2 with IMDB reviews\n",
        "Given below is a command containing few parameters to help Transformers finetune distilgpt2. now, let's understand what these parameters mean\n",
        "\n",
        "1) output_dir: It is the weights_dir we made where our finetuned model will be stored in the form of checkpoints\n",
        "\n",
        "2) model_name_or_path: It tells the kind of model we are currently dealing with\n",
        "\n",
        "3) per_device_train_batch_size: It tells the batch size for each gpu\n",
        "\n",
        "4) do_train: It tells pytorch to start training mode\n",
        "\n",
        "5) train_file: This is where we give the input text data \n",
        "\n",
        "6) num_train_epochs: Number of epochs for finetuning\n",
        "\n",
        "\n",
        "Now, let the training begin..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_rFZRe_2KAW"
      },
      "source": [
        "weights_dir = \"output\""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42gszj3gkUU1"
      },
      "source": [
        "cmd = '''\n",
        "python transformers/examples/language-modeling/run_clm.py \\\n",
        "    --model_name_or_path distilgpt2 \\\n",
        "    --train_file {0} \\\n",
        "    --do_train \\\n",
        "    --num_train_epochs 3 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --per_device_train_batch_size 2 \\\n",
        "    --output_dir {1}\n",
        "'''.format(file_name, weights_dir)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQKT9jlOcnjY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "897ad3c3-39ed-4c15-a0fc-6d7c0c757330"
      },
      "source": [
        "!{cmd}"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-01-09 11:29:38.826121: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "01/09/2021 11:29:40 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "01/09/2021 11:29:40 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='output', overwrite_output_dir=True, do_train=True, do_eval=False, do_predict=False, model_parallel=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Jan09_11-29-40_f6235b34fde1', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fp16_backend='auto', sharded_ddp=False)\n",
            "Using custom data configuration default\n",
            "Reusing dataset text (/root/.cache/huggingface/datasets/text/default-c35390c3e49c7517/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab)\n",
            "[INFO|configuration_utils.py:431] 2021-01-09 11:29:40,885 >> loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
            "[INFO|configuration_utils.py:467] 2021-01-09 11:29:40,886 >> Model config GPT2Config {\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:431] 2021-01-09 11:29:40,902 >> loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
            "[INFO|configuration_utils.py:467] 2021-01-09 11:29:40,903 >> Model config GPT2Config {\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1802] 2021-01-09 11:29:40,992 >> loading file https://huggingface.co/distilgpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/55051ac97dcc32f0a736d21a32a4d42b0d9b90f117ca7c38e65038b04bd5c3f5.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1802] 2021-01-09 11:29:40,992 >> loading file https://huggingface.co/distilgpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/9dfb299b74cdf7601ba7cd3a8073dbdac351caec0ed7ab5849b098b3c8ae3d57.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1802] 2021-01-09 11:29:40,992 >> loading file https://huggingface.co/distilgpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/accb287b5a5396b2597382916b6cc939fdab1366e89475a92338d3971b3d02b7.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|modeling_utils.py:1024] 2021-01-09 11:29:41,074 >> loading weights file https://huggingface.co/distilgpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/43a212e83e76bcb07f45be584cf100676bdbbbe9c13f9e5c1c050049143a832f.a83d881ec4d624fd4b5826dd026e315246c48c67504ff91c0500570e291a54ba\n",
            "[INFO|modeling_utils.py:1140] 2021-01-09 11:29:44,433 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1149] 2021-01-09 11:29:44,433 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-c35390c3e49c7517/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-cceac6dcdf423127.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-c35390c3e49c7517/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-a3d82737c4579dd2.arrow\n",
            "[INFO|trainer.py:388] 2021-01-09 11:29:49,638 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:703] 2021-01-09 11:29:49,640 >> ***** Running training *****\n",
            "[INFO|trainer.py:704] 2021-01-09 11:29:49,640 >>   Num examples = 314\n",
            "[INFO|trainer.py:705] 2021-01-09 11:29:49,641 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:706] 2021-01-09 11:29:49,641 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:707] 2021-01-09 11:29:49,641 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "[INFO|trainer.py:708] 2021-01-09 11:29:49,641 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:709] 2021-01-09 11:29:49,641 >>   Total optimization steps = 471\n",
            "100% 471/471 [04:03<00:00,  1.93it/s][INFO|trainer.py:862] 2021-01-09 11:33:53,314 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'epoch': 3.0}\n",
            "100% 471/471 [04:03<00:00,  1.93it/s]\n",
            "[INFO|trainer.py:1226] 2021-01-09 11:33:53,332 >> Saving model checkpoint to output\n",
            "[INFO|configuration_utils.py:289] 2021-01-09 11:33:53,334 >> Configuration saved in output/config.json\n",
            "[INFO|modeling_utils.py:814] 2021-01-09 11:33:54,459 >> Model weights saved in output/pytorch_model.bin\n",
            "01/09/2021 11:33:54 - INFO - __main__ -   ***** Train results *****\n",
            "Traceback (most recent call last):\n",
            "  File \"transformers/examples/language-modeling/run_clm.py\", line 405, in <module>\n",
            "    main()\n",
            "  File \"transformers/examples/language-modeling/run_clm.py\", line 371, in main\n",
            "    for key, value in sorted(train_result.metrics.items()):\n",
            "AttributeError: 'TrainOutput' object has no attribute 'metrics'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKl2zr2Gm2Db"
      },
      "source": [
        "Although, Huggingface provides a run_generation.py file for language generation. Running it from a command (as it takes the input), makes it load the model and the tokenizer everytime you run the file which slows downs generation. To reduce the I/O overhead, I have restructured the run_generation.py file in the following code which only loads the model and tokenizer once in a model and a tokenizer object and we can use these objects to generate text over and over again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7_-L9gGziiv"
      },
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "def get_model_tokenizer(weights_dir, device = 'cuda'):\n",
        "    print(\"Loading Model ...\")\n",
        "    model = GPT2LMHeadModel.from_pretrained(weights_dir)\n",
        "    model.to('cuda')\n",
        "    print(\"Model Loaded ...\")\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(weights_dir)\n",
        "    return model, tokenizer\n",
        "\n",
        "def generate_messages(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt_text,\n",
        "    stop_token,\n",
        "    length,\n",
        "    num_return_sequences,\n",
        "    temperature = 0.7,\n",
        "    k=20,\n",
        "    p=0.9,\n",
        "    repetition_penalty = 1.0,\n",
        "    device = 'cuda'\n",
        "):\n",
        "\n",
        "    MAX_LENGTH = int(10000)\n",
        "    def adjust_length_to_model(length, max_sequence_length):\n",
        "        if length < 0 and max_sequence_length > 0:\n",
        "            length = max_sequence_length\n",
        "        elif 0 < max_sequence_length < length:\n",
        "            length = max_sequence_length  # No generation bigger than model size\n",
        "        elif length < 0:\n",
        "            length = MAX_LENGTH  # avoid infinite loop\n",
        "        return length\n",
        "        \n",
        "    length = adjust_length_to_model(length=length, max_sequence_length=model.config.max_position_embeddings)\n",
        "\n",
        "    encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n",
        "\n",
        "    encoded_prompt = encoded_prompt.to(device)\n",
        "\n",
        "    output_sequences = model.generate(\n",
        "            input_ids=encoded_prompt,\n",
        "            max_length=length + len(encoded_prompt[0]),\n",
        "            temperature=temperature,\n",
        "            top_k=k,\n",
        "            top_p=p,\n",
        "            repetition_penalty=repetition_penalty,\n",
        "            do_sample=True,\n",
        "            num_return_sequences=num_return_sequences,\n",
        "        )\n",
        "\n",
        "    if len(output_sequences.shape) > 2:\n",
        "        output_sequences.squeeze_()\n",
        "\n",
        "    generated_sequences = []\n",
        "\n",
        "    for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "        #print(\"=== GENERATED SEQUENCE {} ===\".format(generated_sequence_idx + 1))\n",
        "        generated_sequence = generated_sequence.tolist()\n",
        "\n",
        "        # Decode text\n",
        "        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "\n",
        "        # Remove all text after the stop token\n",
        "        text = text[: text.find(stop_token) if stop_token else None]\n",
        "\n",
        "        # Add the prompt at the beginning of the sequence. Remove the excess text that was used for pre-processing\n",
        "        total_sequence = (\n",
        "            prompt_text + text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]\n",
        "        )\n",
        "\n",
        "        generated_sequences.append(total_sequence)\n",
        "    return generated_sequences\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3fwyPATznLp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dccf7df-e149-49e7-b7be-67fa55a327f7"
      },
      "source": [
        "\n",
        "model, tokenizer = get_model_tokenizer(weights_dir, device = 'cuda')\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Model ...\n",
            "Model Loaded ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqpCwzTUz0wZ"
      },
      "source": [
        "temperature = 1.0\n",
        "k=400\n",
        "p=0.9\n",
        "repetition_penalty = 1.0\n",
        "num_return_sequences = 5\n",
        "length = 1000\n",
        "stop_token = '|EndOfText|'\n",
        "prompt_text = \"this is\""
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQv52LcT0Iyi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2bc9fb7-0b3a-49c2-d9b5-c232062ac2b5"
      },
      "source": [
        "%%time\n",
        "generate_messages(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt_text,\n",
        "    stop_token,\n",
        "    length,\n",
        "    num_return_sequences,\n",
        "    temperature = temperature,\n",
        "    k=k,\n",
        "    p=p,\n",
        "    repetition_penalty = repetition_penalty\n",
        ")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 9.84 s, sys: 1.85 s, total: 11.7 s\n",
            "Wall time: 11.7 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"this is a really great comic but it isn't. Its one of the best comics you'll ever see. I'll go buy it in the States and be honest, I'll not buy it as a mass sell. \",\n",
              " 'this is a beautiful picture. All the different styles are very enjoyable. These were my only highlight.<br /><br />',\n",
              " 'this is as important to us as anything else.\" ',\n",
              " \"this is the best video of my life. Each scene has more to do with the story of its character and the humor it provides. For me, I'm looking forward to another look back. \",\n",
              " 'this is to be handed back to the dead and must be stumped. The last time we saw \"S\" was in 1981. I might have missed it, but hey I know you guys are getting bored with most of the other \\'artwork\\' and have only read a few books - it\\'s a waste of money. I actually like the concept, but it might make a new impression on some. It could have made a huge difference in the life of others, but I didn\\'t really care - I\\'d have given up on the chance anyway. So, I\\'m glad I won\\'t give a second opinion on \"S\" in that I guess I\\'ll get a different impression on some characters. Perhaps the first time I saw the \"S\" would have seemed like a strange and dramatic one - since I believe I could even refer to it the way it sounds. But again - perhaps I didn\\'t want to have to watch \"S\" in \"A\" so I thought it was best to try it out myself, since the movie was set in the late 1980\\'s, then the movie never aired. So if you liked something you like to see, and don\\'t feel lost seeing it anyway - try it out yourself. ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpXtDNZ4dunQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}